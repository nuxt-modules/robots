---
title: Nuxt Config
description: Learn how to configure Nuxt Robots using nuxt.config.
---

## `enabled: boolean`{lang="ts"}

- Default: `true`{lang="ts"}

Conditionally toggle the module.

## `allow: string[]`{lang="ts"}

- Default: `[]`{lang="ts"}

Allow paths to be indexed for the `*` user-agent (all robots).

## `disallow: string[]`{lang="ts"}

- Default: `[]`{lang="ts"}

Disallow paths from being indexed for the `*` user-agent (all robots).

## `metaTag: boolean`{lang="ts"}

- Default: `true`{lang="ts"}

Whether to add a `<meta name="robots" ...>` tag to the `<head>` of each page.

## `groups: RobotsGroupInput[]`{lang="ts"}

- Default: `[]`{lang="ts"}

Define more granular rules for the robots.txt. Each group is a set of rules for specific user agent(s).

```ts twoslash
export default defineNuxtConfig({
  robots: {
    groups: [
      {
        userAgent: ['AdsBot-Google-Mobile', 'AdsBot-Google-Mobile-Apps'],
        disallow: ['/admin'],
        allow: ['/admin/login'],
        comments: 'Allow Google AdsBot to index the login page but no-admin pages'
      },
    ]
  }
})
```

## `sitemap: MaybeArray<string>`{lang="ts"}

- Default: `[]`{lang="ts"}

The sitemap URL(s) for the site. If you have multiple sitemaps, you can provide an array of URLs.

You must either define the runtime config `siteUrl` or provide the sitemap as absolute URLs.

```ts
export default defineNuxtConfig({
  robots: {
    sitemap: [
      '/sitemap-one.xml',
      '/sitemap-two.xml',
    ],
  },
})
```

## `robotsEnabledValue: string`{lang="ts"}

- Default: `'index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1'`{lang="ts"}

The value to use when the page is indexable.

## `robotsDisabledValue: string`{lang="ts"}

- Type: `string`
- Default: `'noindex, nofollow'`

The value to use when the page is not indexable.

## `mergeWithRobotsTxtPath: boolean | string`{lang="ts"}

- Default: `true`{lang="ts"}

Specify a robots.txt path to merge the config from, relative to the root directory.

When set to `true`, the default path of `<publicDir>/robots.txt` will be used.

When set to `false`, no merging will occur.

## `blockNonSeoBots: boolean`{lang="ts"}

- Default: `false`{lang="ts"}

Blocks some non-SEO bots from crawling your site. This is not a replacement for a full-blown bot management solution, but it can help to reduce the load on your server.

See [const.ts](https://github.com/nuxt-modules/robots/blob/main/src/const.ts#L6) for the list of bots that are blocked.

```ts twoslash
export default defineNuxtConfig({
  robots: {
    blockNonSeoBots: true
  }
})
```

## `robotsTxt: boolean`{lang="ts"}

- Default: `true`{lang="ts"}

Whether to generate a `robots.txt` file. Useful for disabling when using a base URL.

## `cacheControl: string | false`{lang="ts"}

- Default: `'max-age=14400, must-revalidate'`{lang="ts"}

Configure the Cache-Control header for the robots.txt file. By default it's cached for
4 hours and must be revalidated.

Providing false will set the header to `'no-store'`{lang="ts"}.

```ts twoslash [nuxt.config.ts]
export default defineNuxtConfig({
  robots: {
    cacheControl: 'max-age=14400, must-revalidate'
  }
})
```

## `disableNuxtContentIntegration: boolean`{lang="ts"}

- Default: `undefined`{lang="ts"}

Whether to disable the [Nuxt Content Integration](/docs/robots/guides/content).

## `debug: boolean`{lang="ts"}

- Type: `boolean`{lang="ts"}
- Default: `false`{lang="ts"}

Enables debug logs and a debug endpoint.

## `credits: boolean`{lang="ts"}

- Default: `true`{lang="ts"}

Control the module credit comment in the generated robots.txt file.

```robots-txt [robots.txt]
# START nuxt-robots (indexable) <- credits
# ...
# END nuxt-robots <- credits
```

```ts twoslash [nuxt.config.ts]
export default defineNuxtConfig({
  robots: {
    credits: false
  }
})
```

## `disallowNonIndexableRoutes: boolean`{lang="ts"}

**⚠️ Deprecated**: Explicitly disallow routes in the `/robots.txt` file if you don't want them to be accessible.

- Default: `'false'`{lang="ts"}

Should route rules which disallow indexing be added to the `/robots.txt` file.
