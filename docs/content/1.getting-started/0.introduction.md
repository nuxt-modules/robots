---
title: 'Nuxt Robots'
description: 'Nuxt Robots manages the robots crawling your site with minimal config and best practice defaults.'
navigation:
  title: 'Introduction'
---

## Why use Nuxt Robots?

Nuxt Robots is a module for configuring the robots crawling your site with minimal config and best practice defaults.

The core feature of the module is:
- Telling [crawlers](https://developers.google.com/search/docs/crawling-indexing/overview-google-crawlers) which paths they can and cannot access using a [robots.txt](https://developers.google.com/search/docs/crawling-indexing/robots/intro) file.
- Telling [search engine crawlers](https://developers.google.com/search/docs/crawling-indexing/googlebot) what they can show in search results from your site using a `<meta name="robots" content="index">`{lang="html"} `X-Robots-Tag` HTTP header.

New to robots or SEO? Check out the [Controlling Web Crawlers](/learn/controlling-crawlers) guide to learn more about why you might
need these features.

:LearnLabel{label="Conquering Web Crawlers" to="/learn/controlling-crawlers" icon="i-ph-robot-duotone"}

While it's simple to create your own robots.txt file, the module makes sure your non-production environments get disabled from indexing. This is important to avoid duplicate content issues and to avoid search engines serving your development or staging content to users.

The module also acts as an integration point for other modules. For example:
- [Nuxt Sitemap](/docs/sitemap/getting-started/introduction) ensures pages you've marked as disallowed from indexing are excluded from the sitemap.
- [Nuxt Schema.org](/docs/schema-org/getting-started/introduction) skips rendering Schema.org data if the page is marked as excluded from indexing.

Ready to get started? Check out the [installation guide](/docs/robots/getting-started/installation).

## Features

Nuxt Robots manages the robots crawling your site with minimal config and best practice defaults.

### ü§ñ Robots.txt Config

Configuring the rules is as simple as adding a production robots.txt file to your project.

- [Config using Robots.txt](/docs/robots/guides/robots-txt)

### üóø X-Robots-Tag Header, Meta Tag

Ensures pages that should not be indexed are not indexed with the following:
- `X-Robots-Tag` header
- `<meta name="robots" ...>` meta tag

Both enabled by default.

- [How it works](/docs/robots/guides/how-it-works)

### üïµÔ∏è Bot Detection

Detect and classify bots with server-side header analysis and optional client-side browser fingerprinting.

Identify search engines, social media crawlers, AI bots, automation tools, and security scanners to optimize your application for both human users and automated agents.

- [Bot Detection Guide](/docs/robots/guides/bot-detection)

### üîí Production only indexing

The module uses [Nuxt Site Config](/docs/site-config/getting-started/background) to determine if the site is in production mode.

It will disable non-production environments from being indexed, avoiding duplicate content issues.

- [Environment Config](/docs/robots/guides/disable-indexing)

### üîÑ Easy and powerful configuration

Use route rules to easily target subsets of your site.
When you need even more control, use the runtime Nitro hooks to dynamically configure your robots rules.

- [Route Rules](/docs/robots/guides/route-rules)
- [Nitro Hooks](/docs/robots/nitro-api/nitro-hooks)

### üåé I18n Support

Will automatically fix any non-localised paths within your `allow` and `disallow` rules.

- [I18n Integration](/docs/robots/advanced/i18n)

::callout{icon="i-heroicons-wrench" to="/tools/robots-txt-generator"}
**Test your robots.txt** - Use our free [Robots.txt Generator & Tester](/tools/robots-txt-generator) to validate rules and test against 50+ user agents.
::
