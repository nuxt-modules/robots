---
title: Config using Robots.txt
description: Configure your generated robots.txt file with a robots.txt file.
---

## Introduction

The [robots.txt standard](https://developers.google.com/search/docs/crawling-indexing/robots/create-robots-txt) is important for search engines
to understand which pages to crawl and index on your site.

New to robots.txt? Check out the [Robots.txt Guide](/learn/controlling-crawlers/robots-txt) to learn more.

To match closer to the robots standard, Nuxt Robots recommends configuring the module by using a `robots.txt`, which will be parsed, validated, configuring the module.

If you need programmatic control, you can configure the module using [nuxt.config.ts](/docs/robots/guides/nuxt-config),
[Route Rules](/docs/robots/guides/route-rules) and [Nitro hooks](/docs/robots/nitro-api/nitro-hooks).

## Creating a `robots.txt` file

You can place your file in any location; the easiest is to use: `<rootDir>/public/_robots.txt`.

Additionally, the following paths are supported by default:

```bash [Example File Structure]
# root directory
robots.txt
# asset folders
assets/
├── robots.txt
# pages folder
pages/
├── robots.txt
├── _dir/
│   └── robots.txt
# public folder
public/
├── _robots.txt
├── _dir/
│   └── robots.txt
```

### Custom paths

If you find this too restrictive,
you can use the `mergeWithRobotsTxtPath` config to load your `robots.txt` file from any path.

```ts
export default defineNuxtConfig({
  robots: {
    mergeWithRobotsTxtPath: 'assets/custom/robots.txt'
  }
})
```

## Parsed robots.txt

The following rules are parsed from your `robots.txt` file:

- `User-agent` - The user-agent to apply the rules to.
- `Disallow` - An array of paths to disallow for the user-agent.
- `Allow` - An array of paths to allow for the user-agent.
- `Sitemap` - An array of sitemap URLs to include in the generated sitemap.
- `Content-Usage` - Content Signals for expressing AI usage preferences (see [Content Signals](#content-signals) below).

This parsed data will be shown for environments that are `indexable`.

## Content Signals

Content Signals allow you to express preferences about how AI systems should interact with your content using the `Content-Usage` directive.

```txt [robots.txt]
User-agent: *
Allow: /
Content-Usage: ai=n
Content-Usage: /public/ train-ai=y
Content-Usage: /restricted/ ai=n train-ai=n
```

See the emerging [IETF AI Preferences specification](https://datatracker.ietf.org/doc/draft-ietf-aipref-attach/) for more details.

## Conflicting `public/robots.txt`

To ensure other modules can integrate with your generated robots file, you must not have a `robots.txt` file in your `public` folder.

If you do, it will be moved to `<rootDir>/public/_robots.txt` and merged with the generated file.
