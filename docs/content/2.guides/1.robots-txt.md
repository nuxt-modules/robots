---
title: Config using Robots.txt
description: Configure your generated robots.txt file with a robots.txt file.
---

## Introduction

The [robots.txt standard](https://developers.google.com/search/docs/crawling-indexing/robots/create-robots-txt) is important for search engines
to understand which pages to crawl and index on your site.

New to robots.txt? Check out the [Robots.txt Guide](/learn/controlling-crawlers/robots-txt) to learn more.

To match closer to the robots standard, Nuxt Robots recommends configuring the module by using a `robots.txt`, which will be parsed, validated, configuring the module.

If you need programmatic control, you can configure the module using [nuxt.config.ts](/docs/robots/guides/nuxt-config),
[Route Rules](/docs/robots/guides/route-rules) and [Nitro hooks](/docs/robots/nitro-api/nitro-hooks).

## Creating a `robots.txt` file

You can place your file in any location. The easiest and recommended location is: `<rootDir>/public/_robots.txt`

**Note:** The file is named `_robots.txt` (with an underscore prefix) in the `public` folder to prevent conflicts with the auto-generated file. The module will automatically merge this file with generated rules.

**Quick Start:**

1. Create a file named `_robots.txt` in your `public` folder
2. Add your robots.txt rules to this file
3. The module will automatically detect and merge it with the generated robots.txt

**Example `public/_robots.txt`:**

```txt [public/_robots.txt]
User-agent: *
Allow: /

Sitemap: https://example.com/sitemap.xml
```

Additionally, the following paths are supported by default:

```bash [Example File Structure]
# root directory
robots.txt
# asset folders
assets/
├── robots.txt
# pages folder
pages/
├── robots.txt
├── _dir/
│   └── robots.txt
# public folder
public/
├── _robots.txt
├── _dir/
│   └── robots.txt
```

### Custom paths

If you find this too restrictive,
you can use the `mergeWithRobotsTxtPath` config to load your `robots.txt` file from any path.

```ts
export default defineNuxtConfig({
  robots: {
    mergeWithRobotsTxtPath: 'assets/custom/robots.txt'
  }
})
```

## Parsed robots.txt

The following rules are parsed from your `robots.txt` file:

- `User-agent` - The user-agent to apply the rules to.
- `Disallow` - An array of paths to disallow for the user-agent.
- `Allow` - An array of paths to allow for the user-agent.
- `Sitemap` - An array of sitemap URLs to include in the generated sitemap.
- `Content-Usage` / `Content-Signal` - Directives for expressing AI usage preferences (see [Content Signals](#content-signals) below).

This parsed data will be shown for environments that are `indexable`.

## Content Signals

Content Signals allow you to express preferences about how AI systems should interact with your content. Both `Content-Usage` and `Content-Signal` directives are supported:

### Content-Usage (IETF Standard)

The `Content-Usage` directive follows the [IETF AI Preferences specification](https://datatracker.ietf.org/doc/draft-ietf-aipref-attach/):

```txt [robots.txt]
User-agent: *
Allow: /
Content-Usage: ai=n
Content-Usage: /public/ train-ai=y
Content-Usage: /restricted/ ai=n train-ai=n
```

### Content-Signal (Cloudflare Implementation)

The `Content-Signal` directive is [Cloudflare's implementation](https://blog.cloudflare.com/content-signals-policy/), widely deployed across millions of domains:

```txt [robots.txt]
User-agent: *
Allow: /
Content-Signal: ai-train=no, search=yes, ai-input=yes
```

Both directives are parsed identically and output as `Content-Usage` in the generated robots.txt. Use whichever format matches your preferences or existing tooling.

## Conflicting `public/robots.txt`

To ensure other modules can integrate with your generated robots file, you must not have a `robots.txt` file in your `public` folder.

**Important:** Always use `_robots.txt` (with underscore) instead of `robots.txt` in your public folder.

If you accidentally create a `public/robots.txt` file, the module will automatically:
1. Move it to `<rootDir>/public/_robots.txt`
2. Merge it with the generated file
3. Log a warning in your console

This ensures your custom rules are preserved while allowing the module to function correctly.
